{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6a5403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "from collections import namedtuple, OrderedDict, defaultdict\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import parameter\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchprofile import profile_macs\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "\n",
    "from fast_pytorch_kmeans import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d7b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c954f7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ea35ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(url, model_dir='.', overwrite=False):\n",
    "    import os, sys\n",
    "    from urllib.request import urlretrieve\n",
    "    target_dir = url.split('/')[-1]\n",
    "    model_dir = os.path.expanduser(model_dir)\n",
    "    try:\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "        model_dir = os.path.join(model_dir, target_dir)\n",
    "        cached_file = model_dir\n",
    "        if not os.path.exists(cached_file) or overwrite:\n",
    "            sys.stderr.write('Downloading: \"{}\" to {}\\n'.format(url, cached_file))\n",
    "            urlretrieve(url, cached_file)\n",
    "        return cached_file\n",
    "    except Exception as e:\n",
    "        # remove lock file so download can be executed next time.\n",
    "        os.remove(os.path.join(model_dir, 'download.lock'))\n",
    "        sys.stderr.write('Failed to download from url %s' % url + '\\n' + str(e) + '\\n')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6730cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "  ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "\n",
    "  def __init__(self) -> None:\n",
    "    super().__init__()\n",
    "\n",
    "    layers = []\n",
    "    counts = defaultdict(int)\n",
    "\n",
    "    def add(name: str, layer: nn.Module) -> None:\n",
    "      layers.append((f\"{name}{counts[name]}\", layer))\n",
    "      counts[name] += 1\n",
    "\n",
    "    in_channels = 3\n",
    "    for x in self.ARCH:\n",
    "      if x != 'M':\n",
    "        # conv-bn-relu\n",
    "        add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n",
    "        add(\"bn\", nn.BatchNorm2d(x))\n",
    "        add(\"relu\", nn.ReLU(True))\n",
    "        in_channels = x\n",
    "      else:\n",
    "        # maxpool\n",
    "        add(\"pool\", nn.MaxPool2d(2))\n",
    "    add(\"avgpool\", nn.AvgPool2d(2))\n",
    "    self.backbone = nn.Sequential(OrderedDict(layers))\n",
    "    self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    # backbone: [N, 3, 32, 32] => [N, 512, 2, 2]\n",
    "    x = self.backbone(x)\n",
    "\n",
    "    # avgpool: [N, 512, 2, 2] => [N, 512]\n",
    "    # x = x.mean([2, 3])\n",
    "    x = x.view(x.shape[0], -1)\n",
    "\n",
    "    # classifier: [N, 512] => [N, 10]\n",
    "    x = self.classifier(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a86cdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "  model: nn.Module,\n",
    "  dataloader: DataLoader,\n",
    "  criterion: nn.Module,\n",
    "  optimizer: Optimizer,\n",
    "  scheduler: LambdaLR,\n",
    "  callbacks = None\n",
    ") -> None:\n",
    "  model.train()\n",
    "\n",
    "  for inputs, targets in tqdm(dataloader, desc='train', leave=False):\n",
    "    # Move the data from CPU to GPU\n",
    "    inputs = inputs.cuda()\n",
    "    targets = targets.cuda()\n",
    "\n",
    "    # Reset the gradients (from the last iteration)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward inference\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Backward propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Update optimizer and LR scheduler\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if callbacks is not None:\n",
    "        for callback in callbacks:\n",
    "            callback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcfe9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def evaluate(\n",
    "  model: nn.Module,\n",
    "  dataloader: DataLoader,\n",
    "  extra_preprocess = None\n",
    ") -> float:\n",
    "  model.eval()\n",
    "\n",
    "  num_samples = 0\n",
    "  num_correct = 0\n",
    "\n",
    "  for inputs, targets in tqdm(dataloader, desc=\"eval\", leave=False):\n",
    "    # Move the data from CPU to GPU\n",
    "    inputs = inputs.cuda()\n",
    "    if extra_preprocess is not None:\n",
    "        for preprocess in extra_preprocess:\n",
    "            inputs = preprocess(inputs)\n",
    "\n",
    "    targets = targets.cuda()\n",
    "\n",
    "    # Inference\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Convert logits to class indices\n",
    "    outputs = outputs.argmax(dim=1)\n",
    "\n",
    "    # Update metrics\n",
    "    num_samples += targets.size(0)\n",
    "    num_correct += (outputs == targets).sum()\n",
    "\n",
    "  return (num_correct / num_samples * 100).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337659cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_flops(model, inputs):\n",
    "    num_macs = profile_macs(model, inputs)\n",
    "    return num_macs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a6e837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model: nn.Module, data_width=32):\n",
    "    \"\"\"\n",
    "    calculate the model size in bits\n",
    "    :param data_width: #bits per element\n",
    "    \"\"\"\n",
    "    num_elements = 0\n",
    "    for param in model.parameters():\n",
    "        num_elements += param.numel()\n",
    "    return num_elements * data_width\n",
    "\n",
    "Byte = 8\n",
    "KiB = 1024 * Byte\n",
    "MiB = 1024 * KiB\n",
    "GiB = 1024 * MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0499df",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_url = \"https://hanlab.mit.edu/files/course/labs/vgg.cifar.pretrained.pth\"\n",
    "checkpoint = torch.load(download_url(checkpoint_url), map_location=\"cpu\")\n",
    "model = VGG().cuda()\n",
    "print(f\"=> loading checkpoint '{checkpoint_url}'\")\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "recover_model = lambda : model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b331878",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "transforms = {\n",
    "    \"train\": Compose([\n",
    "        RandomCrop(image_size, padding=4),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "    ]),\n",
    "    \"test\": ToTensor(),\n",
    "}\n",
    "dataset = {}\n",
    "# for split in [\"train\", \"test\"]:\n",
    "#   dataset[split] = CIFAR10(\n",
    "#     root=\"data/cifar10\",\n",
    "#     train=(split == \"train\"),\n",
    "#     download=True,\n",
    "#     transform=transforms[split],\n",
    "#   )\n",
    "dataloader = {}\n",
    "# for split in ['train', 'test']:\n",
    "#   dataloader[split] = DataLoader(\n",
    "#     dataset[split],\n",
    "#     batch_size=512,\n",
    "#     shuffle=(split == 'train'),\n",
    "#     num_workers=0,\n",
    "#     pin_memory=True,\n",
    "#   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390654ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp32_model_accuracy = evaluate(model, dataloader['test'])\n",
    "# fp32_model_size = get_model_size(model)\n",
    "# print(f\"fp32 model has accuracy={fp32_model_accuracy:.2f}%\")\n",
    "# print(f\"fp32 model has size={fp32_model_size/MiB:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71a4596",
   "metadata": {},
   "outputs": [],
   "source": [
    "Codebook = namedtuple('Codebook', ['centroids', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef80519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_quantize(fp32_tensor: torch.Tensor, bitwidth=4, codebook=None):\n",
    "    \"\"\"\n",
    "    quantize tensor using k-means clustering\n",
    "    :param fp32_tensor:\n",
    "    :param bitwidth: [int] quantization bit width, default=4\n",
    "    :param codebook: [Codebook] (the cluster centroids, the cluster label tensor)\n",
    "    :return:\n",
    "        [Codebook = (centroids, labels)]\n",
    "            centroids: [torch.(cuda.)FloatTensor] the cluster centroids\n",
    "            labels: [torch.(cuda.)LongTensor] cluster label tensor\n",
    "    \"\"\"\n",
    "    if codebook is None:\n",
    "        n_clusters = 1 << bitwidth\n",
    "        # use k-means to get the quantization centroids\n",
    "        kmeans = KMeans(n_clusters=n_clusters, mode='euclidean', verbose=0)\n",
    "        labels = kmeans.fit_predict(fp32_tensor.view(-1, 1)).to(torch.long)\n",
    "        centroids = kmeans.centroids.to(torch.float).view(-1)\n",
    "        codebook = Codebook(centroids, labels)\n",
    "    # decode the codebook into k-means quantized tensor for inference\n",
    "    quantized_tensor = codebook.centroids[codebook.labels]\n",
    "    fp32_tensor.set_(quantized_tensor.view_as(fp32_tensor))\n",
    "    return codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e1714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_k_means_quantize(\n",
    "    test_tensor=torch.tensor([\n",
    "        [-0.3747,  0.0874,  0.3200, -0.4868,  0.4404],\n",
    "        [-0.0402,  0.2322, -0.2024, -0.4986,  0.1814],\n",
    "        [ 0.3102, -0.3942, -0.2030,  0.0883, -0.4741],\n",
    "        [-0.1592, -0.0777, -0.3946, -0.2128,  0.2675],\n",
    "        [ 0.0611, -0.1933, -0.4350,  0.2928, -0.1087]]),\n",
    "    bitwidth=2):\n",
    "    def plot_matrix(tensor, ax, title, cmap=ListedColormap(['white'])):\n",
    "        ax.imshow(tensor.cpu().numpy(), vmin=-0.5, vmax=0.5, cmap=cmap)\n",
    "        ax.set_title(title)\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        for i in range(tensor.shape[1]):\n",
    "            for j in range(tensor.shape[0]):\n",
    "                text = ax.text(j, i, f'{tensor[i, j].item():.2f}',\n",
    "                                ha=\"center\", va=\"center\", color=\"k\")\n",
    "        \n",
    "    fig, axes = plt.subplots(1,2, figsize=(8, 12))\n",
    "    ax_left, ax_right = axes.ravel()\n",
    "    \n",
    "    plot_matrix(test_tensor, ax_left, 'original tensor')\n",
    "\n",
    "    num_unique_values_before_quantization = test_tensor.unique().numel()\n",
    "    k_means_quantize(test_tensor, bitwidth=bitwidth)\n",
    "    num_unique_values_after_quantization = test_tensor.unique().numel()\n",
    "    print('* Test k_means_quantize()')\n",
    "    print(f'    target bitwidth: {bitwidth} bits')\n",
    "    print(f'        num unique values before k-means quantization: {num_unique_values_before_quantization}')\n",
    "    print(f'        num unique values after  k-means quantization: {num_unique_values_after_quantization}')\n",
    "    assert num_unique_values_after_quantization == (1 << bitwidth)\n",
    "    print('* Test passed.')\n",
    "\n",
    "    plot_matrix(test_tensor, ax_right, f'{bitwidth}-bit k-means quantized tensor', cmap='tab20c')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528a0274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansQuantizer:\n",
    "    def __init__(self, model : nn.Module, bitwidth=4):\n",
    "        self.codebook = KMeansQuantizer.quantize(model, bitwidth)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def apply(self, model, update_centroids):\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in self.codebook:\n",
    "                if update_centroids:\n",
    "                    update_codebook(param, codebook=self.codebook[name])\n",
    "                self.codebook[name] = k_means_quantize(\n",
    "                    param, codebook=self.codebook[name])\n",
    "\n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def quantize(model: nn.Module, bitwidth=4):\n",
    "        codebook = dict()\n",
    "        if isinstance(bitwidth, dict):\n",
    "            for name, param in model.named_parameters():\n",
    "                if name in bitwidth:\n",
    "                    codebook[name] = k_means_quantize(param, bitwidth=bitwidth[name])\n",
    "        else:\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.dim() > 1:\n",
    "                    codebook[name] = k_means_quantize(param, bitwidth=bitwidth)\n",
    "        return codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb576472",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_k_means_quantize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bff74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizers = dict()\n",
    "for bitwidth in [8, 4, 2]:\n",
    "    recover_model()\n",
    "    print(f'k-means quantizing model into {bitwidth} bits')\n",
    "    quantizer = KMeansQuantizer(model, bitwidth)\n",
    "    quantized_model_size = get_model_size(model, bitwidth)\n",
    "    print(f\"    {bitwidth}-bit k-means quantized model has size={quantized_model_size/MiB:.2f} MiB\")\n",
    "    quantized_model_accuracy = evaluate(model, dataloader['test'])\n",
    "    print(f\"    {bitwidth}-bit k-means quantized model has accuracy={quantized_model_accuracy:.2f}%\")\n",
    "    quantizers[bitwidth] = quantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f243c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_codebook(fp32_tensor: torch.Tensor, codebook: Codebook):\n",
    "    \"\"\"\n",
    "    update the centroids in the codebook using updated fp32_tensor\n",
    "    :param fp32_tensor: [torch.(cuda.)Tensor], the updated weight tensor\n",
    "    :param codebook: [Codebook] (the cluster centroids, the cluster label tensor)\n",
    "    \"\"\"\n",
    "    n_clusters = codebook.centroids.numel()\n",
    "    fp32_tensor = fp32_tensor.view(-1)\n",
    "    for k in range(n_clusters):\n",
    "        codebook.centroids[k] = fp32_tensor[codebook.labels == k].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef366221",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_drop_threshold = 0.5\n",
    "quantizers_before_finetune = copy.deepcopy(quantizers)\n",
    "quantizers_after_finetune = quantizers\n",
    "\n",
    "for bitwidth in [8, 4, 2]:\n",
    "    recover_model()\n",
    "    quantizer = quantizers[bitwidth]\n",
    "    print(f'k-means quantizing model into {bitwidth} bits')\n",
    "    quantizer.apply(model, update_centroids=False)\n",
    "    quantized_model_size = get_model_size(model, bitwidth)\n",
    "    print(f\"    {bitwidth}-bit k-means quantized model has size={quantized_model_size/MiB:.2f} MiB\")\n",
    "    quantized_model_accuracy = evaluate(model, dataloader['test'])\n",
    "    print(f\"    {bitwidth}-bit k-means quantized model has accuracy={quantized_model_accuracy:.2f}% before quantization-aware training \")\n",
    "    accuracy_drop = fp32_model_accuracy - quantized_model_accuracy\n",
    "    if accuracy_drop > accuracy_drop_threshold:\n",
    "        print(f\"        Quantization-aware training due to accuracy drop={accuracy_drop:.2f}% is larger than threshold={accuracy_drop_threshold:.2f}%\")\n",
    "        num_finetune_epochs = 5\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_finetune_epochs)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        best_accuracy = 0\n",
    "        epoch = num_finetune_epochs\n",
    "        while accuracy_drop > accuracy_drop_threshold and epoch > 0:\n",
    "            train(model, dataloader['train'], criterion, optimizer, scheduler,\n",
    "                  callbacks=[lambda: quantizer.apply(model, update_centroids=True)])\n",
    "            model_accuracy = evaluate(model, dataloader['test'])\n",
    "            is_best = model_accuracy > best_accuracy\n",
    "            best_accuracy = max(model_accuracy, best_accuracy)\n",
    "            print(f'        Epoch {num_finetune_epochs-epoch} Accuracy {model_accuracy:.2f}% / Best Accuracy: {best_accuracy:.2f}%')\n",
    "            accuracy_drop = fp32_model_accuracy - best_accuracy\n",
    "            epoch -= 1\n",
    "    else:\n",
    "        print(f\"        No need for quantization-aware training since accuracy drop={accuracy_drop:.2f}% is smaller than threshold={accuracy_drop_threshold:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca70db7d",
   "metadata": {},
   "source": [
    "# linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252a6465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantized_range(bitwidth):\n",
    "    quantized_max = (1 << (bitwidth - 1)) - 1\n",
    "    quantized_min = -(1 << (bitwidth - 1))\n",
    "    return quantized_min, quantized_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d064e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_quantize(fp_tensor, bitwidth, scale, zero_point, dtype=torch.int8) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    linear quantization for single fp_tensor\n",
    "      from\n",
    "        fp_tensor = (quantized_tensor - zero_point) * scale\n",
    "      we have,\n",
    "        quantized_tensor = int(round(fp_tensor / scale)) + zero_point\n",
    "    :param tensor: [torch.(cuda.)FloatTensor] floating tensor to be quantized\n",
    "    :param bitwidth: [int] quantization bit width\n",
    "    :param scale: [torch.(cuda.)FloatTensor] scaling factor\n",
    "    :param zero_point: [torch.(cuda.)IntTensor] the desired centroid of tensor values\n",
    "    :return:\n",
    "        [torch.(cuda.)FloatTensor] quantized tensor whose values are integers\n",
    "    \"\"\"\n",
    "    assert(fp_tensor.dtype == torch.float)\n",
    "    assert(isinstance(scale, float) or \n",
    "           (scale.dtype == torch.float and scale.dim() == fp_tensor.dim()))\n",
    "    assert(isinstance(zero_point, int) or \n",
    "           (zero_point.dtype == dtype and zero_point.dim() == fp_tensor.dim()))\n",
    "\n",
    "    scaled_tensor = fp_tensor / scale\n",
    "    rounded_tensor = torch.round(scaled_tensor)\n",
    "\n",
    "    rounded_tensor = rounded_tensor.to(dtype)\n",
    "\n",
    "    shifted_tensor = rounded_tensor + zero_point\n",
    "\n",
    "    quantized_min, quantized_max = get_quantized_range(bitwidth)\n",
    "    quantized_tensor = shifted_tensor.clamp_(quantized_min, quantized_max)\n",
    "    return quantized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f785d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_linear_quantize(\n",
    "    test_tensor=torch.tensor([\n",
    "        [ 0.0523,  0.6364, -0.0968, -0.0020,  0.1940],\n",
    "        [ 0.7500,  0.5507,  0.6188, -0.1734,  0.4677],\n",
    "        [-0.0669,  0.3836,  0.4297,  0.6267, -0.0695],\n",
    "        [ 0.1536, -0.0038,  0.6075,  0.6817,  0.0601],\n",
    "        [ 0.6446, -0.2500,  0.5376, -0.2226,  0.2333]]),\n",
    "    quantized_test_tensor=torch.tensor([\n",
    "        [-1,  1, -1, -1,  0],\n",
    "        [ 1,  1,  1, -2,  0],\n",
    "        [-1,  0,  0,  1, -1],\n",
    "        [-1, -1,  1,  1, -1],\n",
    "        [ 1, -2,  1, -2,  0]], dtype=torch.int8),\n",
    "    real_min=-0.25, real_max=0.75, bitwidth=2, scale=1/3, zero_point=-1):\n",
    "    def plot_matrix(tensor, ax, title, vmin=0, vmax=1, cmap=ListedColormap(['white'])):\n",
    "        ax.imshow(tensor.cpu().numpy(), vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "        ax.set_title(title)\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        for i in range(tensor.shape[0]):\n",
    "            for j in range(tensor.shape[1]):\n",
    "                datum = tensor[i, j].item()\n",
    "                if isinstance(datum, float):\n",
    "                    text = ax.text(j, i, f'{datum:.2f}',\n",
    "                                    ha=\"center\", va=\"center\", color=\"k\")\n",
    "                else:\n",
    "                    text = ax.text(j, i, f'{datum}',\n",
    "                                    ha=\"center\", va=\"center\", color=\"k\")\n",
    "    quantized_min, quantized_max = get_quantized_range(bitwidth)\n",
    "    fig, axes = plt.subplots(1,3, figsize=(10, 32))\n",
    "    plot_matrix(test_tensor, axes[0], 'original tensor', vmin=real_min, vmax=real_max)\n",
    "    _quantized_test_tensor = linear_quantize(\n",
    "        test_tensor, bitwidth=bitwidth, scale=scale, zero_point=zero_point)\n",
    "    _reconstructed_test_tensor = scale * (_quantized_test_tensor.float() - zero_point)\n",
    "    print('* Test linear_quantize()')\n",
    "    print(f'    target bitwidth: {bitwidth} bits')\n",
    "    print(f'        scale: {scale}')\n",
    "    print(f'        zero point: {zero_point}')\n",
    "    assert _quantized_test_tensor.equal(quantized_test_tensor)\n",
    "    print('* Test passed.')\n",
    "    plot_matrix(_quantized_test_tensor, axes[1], f'2-bit linear quantized tensor',\n",
    "                vmin=quantized_min, vmax=quantized_max, cmap='tab20c')\n",
    "    plot_matrix(_reconstructed_test_tensor, axes[2], f'reconstructed tensor',\n",
    "                vmin=real_min, vmax=real_max, cmap='tab20c')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df03247",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_linear_quantize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7948f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantization_scale_and_zero_point(fp_tensor, bitwidth):\n",
    "    \"\"\"\n",
    "    get quantization scale for single tensor\n",
    "    :param fp_tensor: [torch.(cuda.)Tensor] floating tensor to be quantized\n",
    "    :param bitwidth: [int] quantization bit width\n",
    "    :return:\n",
    "        [float] scale\n",
    "        [int] zero_point\n",
    "    \"\"\"\n",
    "    quantized_min, quantized_max = get_quantized_range(bitwidth)\n",
    "    fp_max = fp_tensor.max().item()\n",
    "    fp_min = fp_tensor.min().item()\n",
    "\n",
    "    scale = (fp_max - fp_min) / (quantized_max - quantized_min)\n",
    "    zero_point = int(round(quantized_min - fp_min / scale))\n",
    "\n",
    "    # clip the zero_point to fall in [quantized_min, quantized_max]\n",
    "    if zero_point < quantized_min:\n",
    "        zero_point = quantized_min\n",
    "    elif zero_point > quantized_max:\n",
    "        zero_point = quantized_max\n",
    "    else: # convert from float to int using round()\n",
    "        zero_point = round(zero_point)\n",
    "    return scale, int(zero_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc3dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_quantize_feature(fp_tensor, bitwidth):\n",
    "    \"\"\"\n",
    "    linear quantization for feature tensor\n",
    "    :param fp_tensor: [torch.(cuda.)Tensor] floating feature to be quantized\n",
    "    :param bitwidth: [int] quantization bit width\n",
    "    :return:\n",
    "        [torch.(cuda.)Tensor] quantized tensor\n",
    "        [float] scale tensor\n",
    "        [int] zero point\n",
    "    \"\"\"\n",
    "    scale, zero_point = get_quantization_scale_and_zero_point(fp_tensor, bitwidth)\n",
    "    quantized_tensor = linear_quantize(fp_tensor, bitwidth, scale, zero_point)\n",
    "    return quantized_tensor, scale, zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352aff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weight_distribution(model, bitwidth=32):\n",
    "    # bins = (1 << bitwidth) if bitwidth <= 8 else 256\n",
    "    if bitwidth <= 8:\n",
    "        qmin, qmax = get_quantized_range(bitwidth)\n",
    "        bins = np.arange(qmin, qmax + 2)\n",
    "        align = 'left'\n",
    "    else:\n",
    "        bins = 256\n",
    "        align = 'mid'\n",
    "    fig, axes = plt.subplots(3,3, figsize=(10, 6))\n",
    "    axes = axes.ravel()\n",
    "    plot_index = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.dim() > 1:\n",
    "            ax = axes[plot_index]\n",
    "            ax.hist(param.detach().view(-1).cpu(), bins=bins, density=True, \n",
    "                    align=align, color = 'blue', alpha = 0.5,\n",
    "                    edgecolor='black' if bitwidth <= 4 else None)\n",
    "            if bitwidth <= 4:\n",
    "                quantized_min, quantized_max = get_quantized_range(bitwidth)\n",
    "                ax.set_xticks(np.arange(start=quantized_min, stop=quantized_max+1))\n",
    "            ax.set_xlabel(name)\n",
    "            ax.set_ylabel('density')\n",
    "            plot_index += 1\n",
    "    fig.suptitle(f'Histogram of Weights (bitwidth={bitwidth} bits)')\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.925)\n",
    "    plt.show()\n",
    "\n",
    "recover_model()\n",
    "plot_weight_distribution(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cecd2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantization_scale_for_weight(weight, bitwidth):\n",
    "    \"\"\"\n",
    "    get quantization scale for single tensor of weight\n",
    "    :param weight: [torch.(cuda.)Tensor] floating weight to be quantized\n",
    "    :param bitwidth: [integer] quantization bit width\n",
    "    :return:\n",
    "        [floating scalar] scale\n",
    "    \"\"\"\n",
    "    # we just assume values in weight are symmetric\n",
    "    # we also always make zero_point 0 for weight\n",
    "    fp_max = max(weight.abs().max().item(), 5e-7)\n",
    "    _, quantized_max = get_quantized_range(bitwidth)\n",
    "    return fp_max / quantized_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a934b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_quantize_weight_per_channel(tensor, bitwidth):\n",
    "    \"\"\"\n",
    "    linear quantization for weight tensor\n",
    "        using different scales and zero_points for different output channels\n",
    "    :param tensor: [torch.(cuda.)Tensor] floating weight to be quantized\n",
    "    :param bitwidth: [int] quantization bit width\n",
    "    :return:\n",
    "        [torch.(cuda.)Tensor] quantized tensor\n",
    "        [torch.(cuda.)Tensor] scale tensor\n",
    "        [int] zero point (which is always 0)\n",
    "    \"\"\"\n",
    "    dim_output_channels = 0\n",
    "    num_output_channels = tensor.shape[dim_output_channels]\n",
    "    scale = torch.zeros(num_output_channels, device=tensor.device)\n",
    "    for oc in range(num_output_channels):\n",
    "        _subtensor = tensor.select(dim_output_channels, oc)\n",
    "        _scale = get_quantization_scale_for_weight(_subtensor, bitwidth)\n",
    "        scale[oc] = _scale\n",
    "    scale_shape = [1] * tensor.dim()\n",
    "    scale_shape[dim_output_channels] = -1\n",
    "    scale = scale.view(scale_shape)\n",
    "    quantized_tensor = linear_quantize(tensor, bitwidth, scale, zero_point=0)\n",
    "    return quantized_tensor, scale, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17da288",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def peek_linear_quantization():\n",
    "    for bitwidth in [4, 2]:\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.dim() > 1:\n",
    "                quantized_param, scale, zero_point = \\\n",
    "                    linear_quantize_weight_per_channel(param, bitwidth)\n",
    "                param.copy_(quantized_param)\n",
    "        plot_weight_distribution(model, bitwidth)\n",
    "        recover_model()\n",
    "\n",
    "peek_linear_quantization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed0556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_quantize_bias_per_output_channel(bias, weight_scale, input_scale):\n",
    "    \"\"\"\n",
    "    linear quantization for single bias tensor\n",
    "        quantized_bias = fp_bias / bias_scale\n",
    "    :param bias: [torch.FloatTensor] bias weight to be quantized\n",
    "    :param weight_scale: [float or torch.FloatTensor] weight scale tensor\n",
    "    :param input_scale: [float] input scale\n",
    "    :return:\n",
    "        [torch.IntTensor] quantized bias tensor\n",
    "    \"\"\"\n",
    "    assert(bias.dim() == 1)\n",
    "    assert(bias.dtype == torch.float)\n",
    "    assert(isinstance(input_scale, float))\n",
    "    if isinstance(weight_scale, torch.Tensor):\n",
    "        assert(weight_scale.dtype == torch.float)\n",
    "        weight_scale = weight_scale.view(-1)\n",
    "        assert(bias.numel() == weight_scale.numel())\n",
    "\n",
    "    bias_scale = weight_scale * input_scale\n",
    "\n",
    "    quantized_bias = linear_quantize(bias, 32, bias_scale,\n",
    "                                     zero_point=0, dtype=torch.int32)\n",
    "    return quantized_bias, bias_scale, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882fb3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_quantized_linear_bias(quantized_bias, quantized_weight, input_zero_point):\n",
    "    \"\"\"\n",
    "    shift quantized bias to incorporate input_zero_point for nn.Linear\n",
    "        shifted_quantized_bias = quantized_bias - Linear(input_zero_point, quantized_weight)\n",
    "    :param quantized_bias: [torch.IntTensor] quantized bias (torch.int32)\n",
    "    :param quantized_weight: [torch.CharTensor] quantized weight (torch.int8)\n",
    "    :param input_zero_point: [int] input zero point\n",
    "    :return:\n",
    "        [torch.IntTensor] shifted quantized bias tensor\n",
    "    \"\"\"\n",
    "    assert(quantized_bias.dtype == torch.int32)\n",
    "    assert(isinstance(input_zero_point, int))\n",
    "    return quantized_bias - quantized_weight.sum(1).to(torch.int32) * input_zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b2107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantized_linear(input, weight, bias, feature_bitwidth, weight_bitwidth,\n",
    "                     input_zero_point, output_zero_point,\n",
    "                     input_scale, weight_scale, output_scale):\n",
    "    \"\"\"\n",
    "    quantized fully-connected layer\n",
    "    :param input: [torch.CharTensor] quantized input (torch.int8)\n",
    "    :param weight: [torch.CharTensor] quantized weight (torch.int8)\n",
    "    :param bias: [torch.IntTensor] shifted quantized bias or None (torch.int32)\n",
    "    :param feature_bitwidth: [int] quantization bit width of input and output\n",
    "    :param weight_bitwidth: [int] quantization bit width of weight\n",
    "    :param input_zero_point: [int] input zero point\n",
    "    :param output_zero_point: [int] output zero point\n",
    "    :param input_scale: [float] input feature scale\n",
    "    :param weight_scale: [torch.FloatTensor] weight per-channel scale\n",
    "    :param output_scale: [float] output feature scale\n",
    "    :return:\n",
    "        [torch.CharIntTensor] quantized output feature (torch.int8)\n",
    "    \"\"\"\n",
    "    assert(input.dtype == torch.int8)\n",
    "    assert(weight.dtype == input.dtype)\n",
    "    assert(bias is None or bias.dtype == torch.int32)\n",
    "    assert(isinstance(input_zero_point, int))\n",
    "    assert(isinstance(output_zero_point, int))\n",
    "    assert(isinstance(input_scale, float))\n",
    "    assert(isinstance(output_scale, float))\n",
    "    assert(weight_scale.dtype == torch.float)\n",
    "\n",
    "    if 'cpu' in input.device.type:\n",
    "        # use 32-b MAC for simplicity\n",
    "        output = torch.nn.functional.linear(input.to(torch.int32), weight.to(torch.int32), bias)\n",
    "    else:\n",
    "        # current version pytorch does not yet support integer-based linear() on GPUs\n",
    "        output = torch.nn.functional.linear(input.float(), weight.float(), bias.float())\n",
    "\n",
    "\n",
    "    output = (input_scale * weight_scale / output_scale) * output.float().transpose(0, 1)\n",
    "\n",
    "    output = output.transpose(0, 1) + output_zero_point\n",
    "\n",
    "\n",
    "    # Make sure all value lies in the bitwidth-bit range\n",
    "    output = output.round().clamp(*get_quantized_range(feature_bitwidth)).to(torch.int8)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4126d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_quantized_fc(\n",
    "    input=torch.tensor([\n",
    "        [0.6118, 0.7288, 0.8511, 0.2849, 0.8427, 0.7435, 0.4014, 0.2794],\n",
    "        [0.3676, 0.2426, 0.1612, 0.7684, 0.6038, 0.0400, 0.2240, 0.4237],\n",
    "        [0.6565, 0.6878, 0.4670, 0.3470, 0.2281, 0.8074, 0.0178, 0.3999],\n",
    "        [0.1863, 0.3567, 0.6104, 0.0497, 0.0577, 0.2990, 0.6687, 0.8626]]),\n",
    "    weight=torch.tensor([\n",
    "        [ 1.2626e-01, -1.4752e-01,  8.1910e-02,  2.4982e-01, -1.0495e-01,\n",
    "         -1.9227e-01, -1.8550e-01, -1.5700e-01],\n",
    "        [ 2.7624e-01, -4.3835e-01,  5.1010e-02, -1.2020e-01, -2.0344e-01,\n",
    "          1.0202e-01, -2.0799e-01,  2.4112e-01],\n",
    "        [-3.8216e-01, -2.8047e-01,  8.5238e-02, -4.2504e-01, -2.0952e-01,\n",
    "          3.2018e-01, -3.3619e-01,  2.0219e-01],\n",
    "        [ 8.9233e-02, -1.0124e-01,  1.1467e-01,  2.0091e-01,  1.1438e-01,\n",
    "         -4.2427e-01,  1.0178e-01, -3.0941e-04],\n",
    "        [-1.8837e-02, -2.1256e-01, -4.5285e-01,  2.0949e-01, -3.8684e-01,\n",
    "         -1.7100e-01, -4.5331e-01, -2.0433e-01],\n",
    "        [-2.0038e-01, -5.3757e-02,  1.8997e-01, -3.6866e-01,  5.5484e-02,\n",
    "          1.5643e-01, -2.3538e-01,  2.1103e-01],\n",
    "        [-2.6875e-01,  2.4984e-01, -2.3514e-01,  2.5527e-01,  2.0322e-01,\n",
    "          3.7675e-01,  6.1563e-02,  1.7201e-01],\n",
    "        [ 3.3541e-01, -3.3555e-01, -4.3349e-01,  4.3043e-01, -2.0498e-01,\n",
    "         -1.8366e-01, -9.1553e-02, -4.1168e-01]]),\n",
    "    bias=torch.tensor([ 0.1954, -0.2756,  0.3113,  0.1149,  0.4274,  0.2429, -0.1721, -0.2502]),\n",
    "    quantized_bias=torch.tensor([ 3, -2,  3,  1,  3,  2, -2, -2], dtype=torch.int32),\n",
    "    shifted_quantized_bias=torch.tensor([-1,  0, -3, -1, -3,  0,  2, -4], dtype=torch.int32),\n",
    "    calc_quantized_output=torch.tensor([\n",
    "        [ 0, -1,  0, -1, -1,  0,  1, -2],\n",
    "        [ 0,  0, -1,  0,  0,  0,  0, -1],\n",
    "        [ 0,  0,  0, -1,  0,  0,  0, -1],\n",
    "        [ 0,  0,  0,  0,  0,  1, -1, -2]], dtype=torch.int8),\n",
    "    bitwidth=2, batch_size=4, in_channels=8, out_channels=8):\n",
    "    def plot_matrix(tensor, ax, title, vmin=0, vmax=1, cmap=ListedColormap(['white'])):\n",
    "        ax.imshow(tensor.cpu().numpy(), vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "        ax.set_title(title)\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        for i in range(tensor.shape[0]):\n",
    "            for j in range(tensor.shape[1]):\n",
    "                datum = tensor[i, j].item()\n",
    "                if isinstance(datum, float):\n",
    "                    text = ax.text(j, i, f'{datum:.2f}',\n",
    "                                    ha=\"center\", va=\"center\", color=\"k\")\n",
    "                else:\n",
    "                    text = ax.text(j, i, f'{datum}',\n",
    "                                    ha=\"center\", va=\"center\", color=\"k\")\n",
    "\n",
    "    output = torch.nn.functional.linear(input, weight, bias)\n",
    "\n",
    "    quantized_weight, weight_scale, weight_zero_point = \\\n",
    "        linear_quantize_weight_per_channel(weight, bitwidth)\n",
    "    quantized_input, input_scale, input_zero_point = \\\n",
    "        linear_quantize_feature(input, bitwidth)\n",
    "    _quantized_bias, bias_scale, bias_zero_point = \\\n",
    "        linear_quantize_bias_per_output_channel(bias, weight_scale, input_scale)\n",
    "    assert _quantized_bias.equal(_quantized_bias)\n",
    "    _shifted_quantized_bias = \\\n",
    "        shift_quantized_linear_bias(quantized_bias, quantized_weight, input_zero_point)\n",
    "    assert _shifted_quantized_bias.equal(shifted_quantized_bias)\n",
    "    quantized_output, output_scale, output_zero_point = \\\n",
    "        linear_quantize_feature(output, bitwidth)\n",
    "\n",
    "    _calc_quantized_output = quantized_linear(\n",
    "        quantized_input, quantized_weight, shifted_quantized_bias,\n",
    "        bitwidth, bitwidth,\n",
    "        input_zero_point, output_zero_point,\n",
    "        input_scale, weight_scale, output_scale)\n",
    "    assert _calc_quantized_output.equal(calc_quantized_output)\n",
    "\n",
    "    reconstructed_weight = weight_scale * (quantized_weight.float() - weight_zero_point)\n",
    "    reconstructed_input = input_scale * (quantized_input.float() - input_zero_point)\n",
    "    reconstructed_bias = bias_scale * (quantized_bias.float() - bias_zero_point)\n",
    "    reconstructed_calc_output = output_scale * (calc_quantized_output.float() - output_zero_point)\n",
    "\n",
    "    fig, axes = plt.subplots(3,3, figsize=(15, 12))\n",
    "    quantized_min, quantized_max = get_quantized_range(bitwidth)\n",
    "    plot_matrix(weight, axes[0, 0], 'original weight', vmin=-0.5, vmax=0.5)\n",
    "    plot_matrix(input.t(), axes[1, 0], 'original input', vmin=0, vmax=1)\n",
    "    plot_matrix(output.t(), axes[2, 0], 'original output', vmin=-1.5, vmax=1.5)\n",
    "    plot_matrix(quantized_weight, axes[0, 1], f'{bitwidth}-bit linear quantized weight',\n",
    "                vmin=quantized_min, vmax=quantized_max, cmap='tab20c')\n",
    "    plot_matrix(quantized_input.t(), axes[1, 1], f'{bitwidth}-bit linear quantized input',\n",
    "                vmin=quantized_min, vmax=quantized_max, cmap='tab20c')\n",
    "    plot_matrix(calc_quantized_output.t(), axes[2, 1], f'quantized output from quantized_linear()',\n",
    "                vmin=quantized_min, vmax=quantized_max, cmap='tab20c')\n",
    "    plot_matrix(reconstructed_weight, axes[0, 2], f'reconstructed weight',\n",
    "                vmin=-0.5, vmax=0.5, cmap='tab20c')\n",
    "    plot_matrix(reconstructed_input.t(), axes[1, 2], f'reconstructed input',\n",
    "                vmin=0, vmax=1, cmap='tab20c')\n",
    "    plot_matrix(reconstructed_calc_output.t(), axes[2, 2], f'reconstructed output',\n",
    "                vmin=-1.5, vmax=1.5, cmap='tab20c')\n",
    "\n",
    "    print('* Test quantized_fc()')\n",
    "    print(f'    target bitwidth: {bitwidth} bits')\n",
    "    print(f'      batch size: {batch_size}')\n",
    "    print(f'      input channels: {in_channels}')\n",
    "    print(f'      output channels: {out_channels}')\n",
    "    print('* Test passed.')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce5e41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_quantized_fc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9dbcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_quantized_conv2d_bias(quantized_bias, quantized_weight, input_zero_point):\n",
    "    \"\"\"\n",
    "    shift quantized bias to incorporate input_zero_point for nn.Conv2d\n",
    "        shifted_quantized_bias = quantized_bias - Conv(input_zero_point, quantized_weight)\n",
    "    :param quantized_bias: [torch.IntTensor] quantized bias (torch.int32)\n",
    "    :param quantized_weight: [torch.CharTensor] quantized weight (torch.int8)\n",
    "    :param input_zero_point: [int] input zero point\n",
    "    :return:\n",
    "        [torch.IntTensor] shifted quantized bias tensor\n",
    "    \"\"\"\n",
    "    assert(quantized_bias.dtype == torch.int32)\n",
    "    assert(isinstance(input_zero_point, int))\n",
    "    return quantized_bias - quantized_weight.sum((1,2,3)).to(torch.int32) * input_zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631716c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantized_conv2d(input, weight, bias, feature_bitwidth, weight_bitwidth,\n",
    "                     input_zero_point, output_zero_point,\n",
    "                     input_scale, weight_scale, output_scale,\n",
    "                     stride, padding, dilation, groups):\n",
    "    \"\"\"\n",
    "    quantized 2d convolution\n",
    "    :param input: [torch.CharTensor] quantized input (torch.int8)\n",
    "    :param weight: [torch.CharTensor] quantized weight (torch.int8)\n",
    "    :param bias: [torch.IntTensor] shifted quantized bias or None (torch.int32)\n",
    "    :param feature_bitwidth: [int] quantization bit width of input and output\n",
    "    :param weight_bitwidth: [int] quantization bit width of weight\n",
    "    :param input_zero_point: [int] input zero point\n",
    "    :param output_zero_point: [int] output zero point\n",
    "    :param input_scale: [float] input feature scale\n",
    "    :param weight_scale: [torch.FloatTensor] weight per-channel scale\n",
    "    :param output_scale: [float] output feature scale\n",
    "    :return:\n",
    "        [torch.(cuda.)CharTensor] quantized output feature\n",
    "    \"\"\"\n",
    "    assert(len(padding) == 4)\n",
    "    assert(input.dtype == torch.int8)\n",
    "    assert(weight.dtype == input.dtype)\n",
    "    assert(bias is None or bias.dtype == torch.int32)\n",
    "    assert(isinstance(input_zero_point, int))\n",
    "    assert(isinstance(output_zero_point, int))\n",
    "    assert(isinstance(input_scale, float))\n",
    "    assert(isinstance(output_scale, float))\n",
    "    assert(weight_scale.dtype == torch.float)\n",
    "\n",
    "    # Step 1: calculate integer-based 2d convolution (8-bit multiplication with 32-bit accumulation)\n",
    "    input = torch.nn.functional.pad(input, padding, 'constant', input_zero_point)\n",
    "    if 'cpu' in input.device.type:\n",
    "        # use 32-b MAC for simplicity\n",
    "        output = torch.nn.functional.conv2d(input.to(torch.int32), weight.to(torch.int32), None, stride, 0, dilation, groups)\n",
    "    else:\n",
    "        # current version pytorch does not yet support integer-based conv2d() on GPUs\n",
    "        output = torch.nn.functional.conv2d(input.float(), weight.float(), None, stride, 0, dilation, groups)\n",
    "        output = output.round().to(torch.int32)\n",
    "    if bias is not None:\n",
    "        output = output + bias.view(1, -1, 1, 1)\n",
    "\n",
    "    ############### YOUR CODE STARTS HERE ###############\n",
    "    # hint: this code block should be the very similar to quantized_linear()\n",
    "\n",
    "    # Step 2: scale the output\n",
    "    #         hint: 1. scales are floating numbers, we need to convert output to float as well\n",
    "    #               2. the shape of weight scale is [oc, 1, 1, 1] while the shape of output is [batch_size, oc, height, width]\n",
    "    output = (input_scale * weight_scale / output_scale) * output.float().transpose(0, 1)\n",
    "\n",
    "    # Step 3: shift output by output_zero_point\n",
    "    #         hint: one line of code\n",
    "    output = output.transpose(0, 1) + output_zero_point\n",
    "    ############### YOUR CODE ENDS HERE #################\n",
    "\n",
    "    # Make sure all value lies in the bitwidth-bit range\n",
    "    output = output.round().clamp(*get_quantized_range(feature_bitwidth)).to(torch.int8)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd42a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_conv_bn(conv, bn):\n",
    "    # modified from https://mmcv.readthedocs.io/en/latest/_modules/mmcv/cnn/utils/fuse_conv_bn.html\n",
    "    assert conv.bias is None\n",
    "\n",
    "    factor = bn.weight.data / torch.sqrt(bn.running_var.data + bn.eps)\n",
    "    conv.weight.data = conv.weight.data * factor.reshape(-1, 1, 1, 1)\n",
    "    conv.bias = nn.Parameter(- bn.running_mean.data * factor + bn.bias.data)\n",
    "\n",
    "    return conv\n",
    "\n",
    "print('Before conv-bn fusion: backbone length', len(model.backbone))\n",
    "#  fuse the batchnorm into conv layers\n",
    "recover_model()\n",
    "model_fused = copy.deepcopy(model)\n",
    "fused_backbone = []\n",
    "ptr = 0\n",
    "while ptr < len(model_fused.backbone):\n",
    "    if isinstance(model_fused.backbone[ptr], nn.Conv2d) and \\\n",
    "        isinstance(model_fused.backbone[ptr + 1], nn.BatchNorm2d):\n",
    "        fused_backbone.append(fuse_conv_bn(\n",
    "            model_fused.backbone[ptr], model_fused.backbone[ptr+ 1]))\n",
    "        ptr += 2\n",
    "    else:\n",
    "        fused_backbone.append(model_fused.backbone[ptr])\n",
    "        ptr += 1\n",
    "model_fused.backbone = nn.Sequential(*fused_backbone)\n",
    "\n",
    "print('After conv-bn fusion: backbone length', len(model_fused.backbone))\n",
    "# sanity check, no BN anymore\n",
    "for m in model_fused.modules():\n",
    "    assert not isinstance(m, nn.BatchNorm2d)\n",
    "\n",
    "#  the accuracy will remain the same after fusion\n",
    "fused_acc = evaluate(model_fused, dataloader['test'])\n",
    "print(f'Accuracy of the fused model={fused_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f69a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add hook to record the min max value of the activation\n",
    "input_activation = {}\n",
    "output_activation = {}\n",
    "\n",
    "def add_range_recoder_hook(model):\n",
    "    import functools\n",
    "    def _record_range(self, x, y, module_name):\n",
    "        x = x[0]\n",
    "        input_activation[module_name] = x.detach()\n",
    "        output_activation[module_name] = y.detach()\n",
    "\n",
    "    all_hooks = []\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear, nn.ReLU)):\n",
    "            all_hooks.append(m.register_forward_hook(\n",
    "                functools.partial(_record_range, module_name=name)))\n",
    "    return all_hooks\n",
    "\n",
    "hooks = add_range_recoder_hook(model_fused)\n",
    "sample_data = iter(dataloader['train']).__next__()[0]\n",
    "model_fused(sample_data.cuda())\n",
    "\n",
    "# remove hooks\n",
    "for h in hooks:\n",
    "    h.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e44a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedConv2d(nn.Module):\n",
    "    def __init__(self, weight, bias, \n",
    "                 input_zero_point, output_zero_point,\n",
    "                 input_scale, weight_scale, output_scale,\n",
    "                 stride, padding, dilation, groups,\n",
    "                 feature_bitwidth=8, weight_bitwidth=8):\n",
    "        super().__init__()\n",
    "        # current version Pytorch does not support IntTensor as nn.Parameter\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.register_buffer('bias', bias)\n",
    "\n",
    "        self.input_zero_point = input_zero_point\n",
    "        self.output_zero_point = output_zero_point\n",
    "\n",
    "        self.input_scale = input_scale\n",
    "        self.register_buffer('weight_scale', weight_scale)\n",
    "        self.output_scale = output_scale\n",
    "\n",
    "        self.stride = stride\n",
    "        self.padding = (padding[1], padding[1], padding[0], padding[0])\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "\n",
    "        self.feature_bitwidth = feature_bitwidth\n",
    "        self.weight_bitwidth = weight_bitwidth\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return quantized_conv2d(\n",
    "            x, self.weight, self.bias, \n",
    "            self.feature_bitwidth, self.weight_bitwidth,\n",
    "            self.input_zero_point, self.output_zero_point,\n",
    "            self.input_scale, self.weight_scale, self.output_scale,\n",
    "            self.stride, self.padding, self.dilation, self.groups\n",
    "            )\n",
    "        \n",
    "class QuantizedLinear(nn.Module):\n",
    "    def __init__(self, weight, bias, \n",
    "                 input_zero_point, output_zero_point,\n",
    "                 input_scale, weight_scale, output_scale,\n",
    "                 feature_bitwidth=8, weight_bitwidth=8):\n",
    "        super().__init__()\n",
    "        # current version Pytorch does not support IntTensor as nn.Parameter\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.register_buffer('bias', bias)\n",
    "\n",
    "        self.input_zero_point = input_zero_point\n",
    "        self.output_zero_point = output_zero_point\n",
    "\n",
    "        self.input_scale = input_scale\n",
    "        self.register_buffer('weight_scale', weight_scale)\n",
    "        self.output_scale = output_scale\n",
    "\n",
    "        self.feature_bitwidth = feature_bitwidth\n",
    "        self.weight_bitwidth = weight_bitwidth\n",
    "\n",
    "    def forward(self, x):\n",
    "        return quantized_linear(\n",
    "            x, self.weight, self.bias, \n",
    "            self.feature_bitwidth, self.weight_bitwidth,\n",
    "            self.input_zero_point, self.output_zero_point,\n",
    "            self.input_scale, self.weight_scale, self.output_scale\n",
    "            )\n",
    "\n",
    "class QuantizedMaxPool2d(nn.MaxPool2d):\n",
    "    def forward(self, x):\n",
    "        # current version PyTorch does not support integer-based MaxPool\n",
    "        return super().forward(x.float()).to(torch.int8)\n",
    "\n",
    "class QuantizedAvgPool2d(nn.AvgPool2d):\n",
    "    def forward(self, x):\n",
    "        # current version PyTorch does not support integer-based AvgPool\n",
    "        return super().forward(x.float()).to(torch.int8)\n",
    "\n",
    "# we use int8 quantization, which is quite popular\n",
    "feature_bitwidth = weight_bitwidth = 8 \n",
    "quantized_model = copy.deepcopy(model_fused)\n",
    "quantized_backbone = []\n",
    "ptr = 0\n",
    "while ptr < len(quantized_model.backbone):\n",
    "    if isinstance(quantized_model.backbone[ptr], nn.Conv2d) and \\\n",
    "        isinstance(quantized_model.backbone[ptr + 1], nn.ReLU):\n",
    "        conv = quantized_model.backbone[ptr]\n",
    "        conv_name = f'backbone.{ptr}'\n",
    "        relu = quantized_model.backbone[ptr + 1]\n",
    "        relu_name = f'backbone.{ptr + 1}'\n",
    "\n",
    "        input_scale, input_zero_point = \\\n",
    "            get_quantization_scale_and_zero_point(\n",
    "                input_activation[conv_name], feature_bitwidth)\n",
    "        \n",
    "        output_scale, output_zero_point = \\\n",
    "            get_quantization_scale_and_zero_point(\n",
    "                output_activation[relu_name], feature_bitwidth)\n",
    "\n",
    "        quantized_weight, weight_scale, weight_zero_point = \\\n",
    "            linear_quantize_weight_per_channel(conv.weight.data, weight_bitwidth)\n",
    "        quantized_bias, bias_scale, bias_zero_point = \\\n",
    "            linear_quantize_bias_per_output_channel(\n",
    "                conv.bias.data, weight_scale, input_scale)\n",
    "        shifted_quantized_bias = \\\n",
    "            shift_quantized_conv2d_bias(quantized_bias, quantized_weight, \n",
    "                                        input_zero_point)\n",
    "            \n",
    "        quantized_conv = QuantizedConv2d(\n",
    "            quantized_weight, shifted_quantized_bias,\n",
    "            input_zero_point, output_zero_point,\n",
    "            input_scale, weight_scale, output_scale,\n",
    "            conv.stride, conv.padding, conv.dilation, conv.groups,\n",
    "            feature_bitwidth=feature_bitwidth, weight_bitwidth=weight_bitwidth\n",
    "        )\n",
    "\n",
    "        quantized_backbone.append(quantized_conv)\n",
    "        ptr += 2\n",
    "    elif isinstance(quantized_model.backbone[ptr], nn.MaxPool2d):\n",
    "        quantized_backbone.append(QuantizedMaxPool2d(\n",
    "            kernel_size=quantized_model.backbone[ptr].kernel_size,\n",
    "            stride=quantized_model.backbone[ptr].stride\n",
    "            ))\n",
    "        ptr += 1\n",
    "    elif isinstance(quantized_model.backbone[ptr], nn.AvgPool2d):\n",
    "        quantized_backbone.append(QuantizedAvgPool2d(\n",
    "            kernel_size=quantized_model.backbone[ptr].kernel_size,\n",
    "            stride=quantized_model.backbone[ptr].stride\n",
    "            ))\n",
    "        ptr += 1\n",
    "    else:\n",
    "        raise NotImplementedError(type(quantized_model.backbone[ptr]))  # should not happen\n",
    "quantized_model.backbone = nn.Sequential(*quantized_backbone)\n",
    "\n",
    "# finally, quantized the classifier\n",
    "fc_name = 'classifier'\n",
    "fc = model.classifier\n",
    "input_scale, input_zero_point = \\\n",
    "    get_quantization_scale_and_zero_point(\n",
    "        input_activation[fc_name], feature_bitwidth)\n",
    "\n",
    "output_scale, output_zero_point = \\\n",
    "    get_quantization_scale_and_zero_point(\n",
    "        output_activation[fc_name], feature_bitwidth)\n",
    "\n",
    "quantized_weight, weight_scale, weight_zero_point = \\\n",
    "    linear_quantize_weight_per_channel(fc.weight.data, weight_bitwidth)\n",
    "quantized_bias, bias_scale, bias_zero_point = \\\n",
    "    linear_quantize_bias_per_output_channel(\n",
    "        fc.bias.data, weight_scale, input_scale)\n",
    "shifted_quantized_bias = \\\n",
    "    shift_quantized_linear_bias(quantized_bias, quantized_weight, \n",
    "                                input_zero_point)\n",
    "            \n",
    "quantized_model.classifier = QuantizedLinear(\n",
    "    quantized_weight, shifted_quantized_bias,\n",
    "    input_zero_point, output_zero_point,\n",
    "    input_scale, weight_scale, output_scale,\n",
    "    feature_bitwidth=feature_bitwidth, weight_bitwidth=weight_bitwidth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27436fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(quantized_model)\n",
    "\n",
    "def extra_preprocess(x):\n",
    "    return (x*255 - 128).clamp(-128, 127).to(torch.int8)\n",
    "\n",
    "int8_model_accuracy = evaluate(quantized_model, dataloader['test'],\n",
    "                               extra_preprocess=[extra_preprocess])\n",
    "print(f\"int8 model has accuracy={int8_model_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c96e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
