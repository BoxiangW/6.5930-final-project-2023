{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bwang/miniconda3/envs/6.5931/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "from collections import namedtuple, OrderedDict, defaultdict\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import parameter\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchprofile import profile_macs\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "\n",
    "from fast_pytorch_kmeans import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "  ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "\n",
    "  def __init__(self) -> None:\n",
    "    super().__init__()\n",
    "\n",
    "    layers = []\n",
    "    counts = defaultdict(int)\n",
    "\n",
    "    def add(name: str, layer: nn.Module) -> None:\n",
    "      layers.append((f\"{name}{counts[name]}\", layer))\n",
    "      counts[name] += 1\n",
    "\n",
    "    in_channels = 3\n",
    "    for x in self.ARCH:\n",
    "      if x != 'M':\n",
    "        # conv-bn-relu\n",
    "        add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n",
    "        add(\"bn\", nn.BatchNorm2d(x))\n",
    "        add(\"relu\", nn.ReLU(True))\n",
    "        in_channels = x\n",
    "      else:\n",
    "        # maxpool\n",
    "        add(\"pool\", nn.MaxPool2d(2))\n",
    "    add(\"avgpool\", nn.AvgPool2d(2))\n",
    "    self.backbone = nn.Sequential(OrderedDict(layers))\n",
    "    self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    # backbone: [N, 3, 32, 32] => [N, 512, 2, 2]\n",
    "    x = self.backbone(x)\n",
    "\n",
    "    # avgpool: [N, 512, 2, 2] => [N, 512]\n",
    "    # x = x.mean([2, 3])\n",
    "    x = x.view(x.shape[0], -1)\n",
    "\n",
    "    # classifier: [N, 512] => [N, 10]\n",
    "    x = self.classifier(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "image_size = 32\n",
    "transforms = {\n",
    "    \"train\": Compose([\n",
    "        RandomCrop(image_size, padding=4),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "    ]),\n",
    "    \"test\": ToTensor(),\n",
    "}\n",
    "dataset = {}\n",
    "for split in [\"train\", \"test\"]:\n",
    "  dataset[split] = CIFAR10(\n",
    "    root=\"data/cifar10\",\n",
    "    train=(split == \"train\"),\n",
    "    download=True,\n",
    "    transform=transforms[split],\n",
    "  )\n",
    "dataloader = {}\n",
    "for split in ['train', 'test']:\n",
    "  dataloader[split] = DataLoader(\n",
    "    dataset[split],\n",
    "    batch_size=512,\n",
    "    shuffle=(split == 'train'),\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def evaluate(\n",
    "  model: nn.Module,\n",
    "  dataloader: DataLoader,\n",
    "  extra_preprocess = None\n",
    ") -> float:\n",
    "  model.eval()\n",
    "\n",
    "  num_samples = 0\n",
    "  num_correct = 0\n",
    "\n",
    "  for inputs, targets in tqdm(dataloader, desc=\"eval\", leave=False):\n",
    "    # Move the data from CPU to GPU\n",
    "    inputs = inputs.cuda()\n",
    "    if extra_preprocess is not None:\n",
    "        for preprocess in extra_preprocess:\n",
    "            inputs = preprocess(inputs)\n",
    "\n",
    "    targets = targets.cuda()\n",
    "\n",
    "    # Inference\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Convert logits to class indices\n",
    "    outputs = outputs.argmax(dim=1)\n",
    "\n",
    "    # Update metrics\n",
    "    num_samples += targets.size(0)\n",
    "    num_correct += (outputs == targets).sum()\n",
    "\n",
    "  return (num_correct / num_samples * 100).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    original model has accuracy=92.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"./model/original.pth\")\n",
    "model.eval()\n",
    "model_accuracy = evaluate(model, dataloader['test'])\n",
    "print(f\"    original model has accuracy={model_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    8-bit k-means quantized model has accuracy=92.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "model8 = torch.load(\"./model/8bit_kmeans_quantized.pth\")\n",
    "model8.eval()\n",
    "quantized_model_accuracy = evaluate(model8, dataloader['test'])\n",
    "print(f\"    8-bit k-means quantized model has accuracy={quantized_model_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    4-bit k-means quantized model has accuracy=92.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "model4 = torch.load(\"./model/4bit_kmeans_quantized.pth\")\n",
    "model4.eval()\n",
    "quantized_model_accuracy = evaluate(model4, dataloader['test'])\n",
    "print(f\"    4-bit k-means quantized model has accuracy={quantized_model_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2-bit k-means quantized model has accuracy=91.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "model2 = torch.load(\"./model/2bit_kmeans_quantized.pth\")\n",
    "model2.eval()\n",
    "quantized_model_accuracy = evaluate(model2, dataloader['test'])\n",
    "print(f\"    2-bit k-means quantized model has accuracy={quantized_model_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantized_range(bitwidth):\n",
    "    quantized_max = (1 << (bitwidth - 1)) - 1\n",
    "    quantized_min = -(1 << (bitwidth - 1))\n",
    "    return quantized_min, quantized_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantized_conv2d(input, weight, bias, feature_bitwidth, weight_bitwidth,\n",
    "                     input_zero_point, output_zero_point,\n",
    "                     input_scale, weight_scale, output_scale,\n",
    "                     stride, padding, dilation, groups):\n",
    "    \"\"\"\n",
    "    quantized 2d convolution\n",
    "    :param input: [torch.CharTensor] quantized input (torch.int8)\n",
    "    :param weight: [torch.CharTensor] quantized weight (torch.int8)\n",
    "    :param bias: [torch.IntTensor] shifted quantized bias or None (torch.int32)\n",
    "    :param feature_bitwidth: [int] quantization bit width of input and output\n",
    "    :param weight_bitwidth: [int] quantization bit width of weight\n",
    "    :param input_zero_point: [int] input zero point\n",
    "    :param output_zero_point: [int] output zero point\n",
    "    :param input_scale: [float] input feature scale\n",
    "    :param weight_scale: [torch.FloatTensor] weight per-channel scale\n",
    "    :param output_scale: [float] output feature scale\n",
    "    :return:\n",
    "        [torch.(cuda.)CharTensor] quantized output feature\n",
    "    \"\"\"\n",
    "    assert(len(padding) == 4)\n",
    "    assert(input.dtype == torch.int8)\n",
    "    assert(weight.dtype == input.dtype)\n",
    "    assert(bias is None or bias.dtype == torch.int32)\n",
    "    assert(isinstance(input_zero_point, int))\n",
    "    assert(isinstance(output_zero_point, int))\n",
    "    assert(isinstance(input_scale, float))\n",
    "    assert(isinstance(output_scale, float))\n",
    "    assert(weight_scale.dtype == torch.float)\n",
    "\n",
    "    input = torch.nn.functional.pad(input, padding, 'constant', input_zero_point)\n",
    "    if 'cpu' in input.device.type:\n",
    "        # use 32-b MAC for simplicity\n",
    "        output = torch.nn.functional.conv2d(input.to(torch.int32), weight.to(torch.int32), None, stride, 0, dilation, groups)\n",
    "    else:\n",
    "        # current version pytorch does not yet support integer-based conv2d() on GPUs\n",
    "        output = torch.nn.functional.conv2d(input.float(), weight.float(), None, stride, 0, dilation, groups)\n",
    "        output = output.round().to(torch.int32)\n",
    "    if bias is not None:\n",
    "        output = output + bias.view(1, -1, 1, 1)\n",
    "\n",
    "    output = (input_scale * weight_scale / output_scale) * output.float().transpose(0, 1)\n",
    "\n",
    "    output = output.transpose(0, 1) + output_zero_point\n",
    "\n",
    "    # Make sure all value lies in the bitwidth-bit range\n",
    "    output = output.round().clamp(*get_quantized_range(feature_bitwidth)).to(torch.int8)\n",
    "    return output\n",
    "\n",
    "\n",
    "def quantized_linear(input, weight, bias, feature_bitwidth, weight_bitwidth,\n",
    "                     input_zero_point, output_zero_point,\n",
    "                     input_scale, weight_scale, output_scale):\n",
    "    \"\"\"\n",
    "    quantized fully-connected layer\n",
    "    :param input: [torch.CharTensor] quantized input (torch.int8)\n",
    "    :param weight: [torch.CharTensor] quantized weight (torch.int8)\n",
    "    :param bias: [torch.IntTensor] shifted quantized bias or None (torch.int32)\n",
    "    :param feature_bitwidth: [int] quantization bit width of input and output\n",
    "    :param weight_bitwidth: [int] quantization bit width of weight\n",
    "    :param input_zero_point: [int] input zero point\n",
    "    :param output_zero_point: [int] output zero point\n",
    "    :param input_scale: [float] input feature scale\n",
    "    :param weight_scale: [torch.FloatTensor] weight per-channel scale\n",
    "    :param output_scale: [float] output feature scale\n",
    "    :return:\n",
    "        [torch.CharIntTensor] quantized output feature (torch.int8)\n",
    "    \"\"\"\n",
    "    assert(input.dtype == torch.int8)\n",
    "    assert(weight.dtype == input.dtype)\n",
    "    assert(bias is None or bias.dtype == torch.int32)\n",
    "    assert(isinstance(input_zero_point, int))\n",
    "    assert(isinstance(output_zero_point, int))\n",
    "    assert(isinstance(input_scale, float))\n",
    "    assert(isinstance(output_scale, float))\n",
    "    assert(weight_scale.dtype == torch.float)\n",
    "\n",
    "    if 'cpu' in input.device.type:\n",
    "        # use 32-b MAC for simplicity\n",
    "        output = torch.nn.functional.linear(input.to(torch.int32), weight.to(torch.int32), bias)\n",
    "    else:\n",
    "        # current version pytorch does not yet support integer-based linear() on GPUs\n",
    "        output = torch.nn.functional.linear(input.float(), weight.float(), bias.float())\n",
    "\n",
    "\n",
    "    output = (input_scale * weight_scale / output_scale) * output.float().transpose(0, 1)\n",
    "\n",
    "    output = output.transpose(0, 1) + output_zero_point\n",
    "\n",
    "\n",
    "    # Make sure all value lies in the bitwidth-bit range\n",
    "    output = output.round().clamp(*get_quantized_range(feature_bitwidth)).to(torch.int8)\n",
    "    return output\n",
    "\n",
    "\n",
    "class QuantizedConv2d(nn.Module):\n",
    "    def __init__(self, weight, bias, \n",
    "                 input_zero_point, output_zero_point,\n",
    "                 input_scale, weight_scale, output_scale,\n",
    "                 stride, padding, dilation, groups,\n",
    "                 feature_bitwidth=8, weight_bitwidth=8):\n",
    "        super().__init__()\n",
    "        # current version Pytorch does not support IntTensor as nn.Parameter\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.register_buffer('bias', bias)\n",
    "\n",
    "        self.input_zero_point = input_zero_point\n",
    "        self.output_zero_point = output_zero_point\n",
    "\n",
    "        self.input_scale = input_scale\n",
    "        self.register_buffer('weight_scale', weight_scale)\n",
    "        self.output_scale = output_scale\n",
    "\n",
    "        self.stride = stride\n",
    "        self.padding = (padding[1], padding[1], padding[0], padding[0])\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "\n",
    "        self.feature_bitwidth = feature_bitwidth\n",
    "        self.weight_bitwidth = weight_bitwidth\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return quantized_conv2d(\n",
    "            x, self.weight, self.bias, \n",
    "            self.feature_bitwidth, self.weight_bitwidth,\n",
    "            self.input_zero_point, self.output_zero_point,\n",
    "            self.input_scale, self.weight_scale, self.output_scale,\n",
    "            self.stride, self.padding, self.dilation, self.groups\n",
    "            )\n",
    "        \n",
    "class QuantizedLinear(nn.Module):\n",
    "    def __init__(self, weight, bias, \n",
    "                 input_zero_point, output_zero_point,\n",
    "                 input_scale, weight_scale, output_scale,\n",
    "                 feature_bitwidth=8, weight_bitwidth=8):\n",
    "        super().__init__()\n",
    "        # current version Pytorch does not support IntTensor as nn.Parameter\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.register_buffer('bias', bias)\n",
    "\n",
    "        self.input_zero_point = input_zero_point\n",
    "        self.output_zero_point = output_zero_point\n",
    "\n",
    "        self.input_scale = input_scale\n",
    "        self.register_buffer('weight_scale', weight_scale)\n",
    "        self.output_scale = output_scale\n",
    "\n",
    "        self.feature_bitwidth = feature_bitwidth\n",
    "        self.weight_bitwidth = weight_bitwidth\n",
    "\n",
    "    def forward(self, x):\n",
    "        return quantized_linear(\n",
    "            x, self.weight, self.bias, \n",
    "            self.feature_bitwidth, self.weight_bitwidth,\n",
    "            self.input_zero_point, self.output_zero_point,\n",
    "            self.input_scale, self.weight_scale, self.output_scale\n",
    "            )\n",
    "\n",
    "class QuantizedMaxPool2d(nn.MaxPool2d):\n",
    "    def forward(self, x):\n",
    "        # current version PyTorch does not support integer-based MaxPool\n",
    "        return super().forward(x.float()).to(torch.int8)\n",
    "\n",
    "class QuantizedAvgPool2d(nn.AvgPool2d):\n",
    "    def forward(self, x):\n",
    "        # current version PyTorch does not support integer-based AvgPool\n",
    "        return super().forward(x.float()).to(torch.int8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    8-bit linear quantized model has accuracy=92.88%\n"
     ]
    }
   ],
   "source": [
    "model8linear = torch.load(\"./model/8bit_linear_quantized.pth\")\n",
    "model8linear.eval()\n",
    "\n",
    "def extra_preprocess(x):\n",
    "    return (x*255 - 128).clamp(-128, 127).to(torch.int8)\n",
    "\n",
    "quantized_model_accuracy = evaluate(model8linear, dataloader['test'],extra_preprocess=[extra_preprocess])\n",
    "print(f\"    8-bit linear quantized model has accuracy={quantized_model_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "6.5931",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
